{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+cYgIdAnSXiePo+opvTvw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saptarshimazumdar/deep-learning-concepts/blob/main/backpropagation/gradient-validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Gradient Checking for Neural Network Training**\n",
        "\n",
        "you need to implement a gradient checking procedure to verify the correctness of backpropagation in a multilayer perceptron (MLP).Consider the following neural network architecture for regression:\n",
        "\n",
        "* Input layer: $x \\in \\mathbb{R}^2$\n",
        "* Hidden layer: 5 neurons with sigmoid activation\n",
        "* Output layer: 1 neuron with linear activation\n",
        "\n",
        "The network equations are:\n",
        "$$h = \\sigma(W^{(1)}x + b^{(1)})$$\n",
        "$$\\hat{y} = W^{(2)}h + b^{(2)}$$\n",
        "\n",
        "The loss function is:\n",
        "$$L = \\frac{1}{2}(y - \\hat{y})^2$$\n",
        "\n",
        "You must generate a small synthetic dataset consisting of 20 samples where:\n",
        "$$x_1, x_2 \\sim \\text{Uniform}(-1, 1)$$\n",
        "and the target output is:\n",
        "$$y = x_1^2 + x_2^2$$\n",
        "\n",
        "1. Implement forward propagation and manual backpropagation using NumPy.\n",
        "2. Implement gradient checking using finite difference approximation:\n",
        "$$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$\n",
        "where $\\epsilon = 10^{-5}$ and $\\theta$ represents any network parameter."
      ],
      "metadata": {
        "id": "1-S5rQDN2EPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Solution**\n",
        "\n",
        "#### Mathematical Derivations\n",
        "**Forward Pass:**\n",
        "* $Z^{(1)} = X W^{(1)} + b^{(1)}$\n",
        "* $H = \\sigma(Z^{(1)})$\n",
        "* $Z^{(2)} = H W^{(2)} + b^{(2)}$\n",
        "* $\\hat{y} = Z^{(2)}$ (Linear activation)\n",
        "* Batch Loss: $L = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2}(\\hat{y}_i - y_i)^2$\n",
        "\n",
        "**Backward Pass (Chain Rule):**\n",
        "* Derivative of Loss w.r.t predictions: $d\\hat{y} = \\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{N} (\\hat{y} - y)$\n",
        "* Gradients for Output Layer ($W^{(2)}, b^{(2)}$):\n",
        "  * $dW^{(2)} = H^T d\\hat{y}$\n",
        "  * $db^{(2)} = \\sum_{i=1}^N d\\hat{y}_i$\n",
        "* Gradients for Hidden Layer ($W^{(1)}, b^{(1)}$):\n",
        "  * $dH = d\\hat{y} (W^{(2)})^T$\n",
        "  * $dZ^{(1)} = dH \\odot H \\odot (1 - H)$ <sup>( using the derivative of the sigmoid function $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ )</sup>\n",
        "  * $dW^{(1)} = X^T dZ^{(1)}$\n",
        "  * $db^{(1)} = \\sum_{i=1}^N dZ^{(1)}_i$"
      ],
      "metadata": {
        "id": "_55xVKKw24CK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4YCfZrL1Kfo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dataset Generation**"
      ],
      "metadata": {
        "id": "kgaQ4SN44UbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "N = 20\n",
        "X = np.random.uniform(-1, 1, size=(N, 2))\n",
        "# Target: y = x1^2 + x2^2 (Shape: (20, 1))\n",
        "y = (X[:, 0]**2 + X[:, 1]**2).reshape(N, 1)"
      ],
      "metadata": {
        "id": "wtJxqEK14P5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron:\n",
        "  def __init__(self, input_dim=2, hidden_dim=5, output_dim=1):\n",
        "    # Initialize weights with xavier distribution (since activation is sigmoid)\n",
        "    # Initialize biases to zero\n",
        "    self.W1 = np.random.normal(0, 2/(input_dim + hidden_dim), (input_dim, hidden_dim))\n",
        "    self.b1 = np.zeros((1, hidden_dim))\n",
        "    self.W2 = np.random.normal(0, 2/(hidden_dim + output_dim), (hidden_dim, output_dim))\n",
        "    self.b2 = np.zeros((1, output_dim))\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.X = X\n",
        "\n",
        "    # Hidden layer\n",
        "    self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "    self.H = self.sigmoid(self.Z1)\n",
        "\n",
        "    # Output layer\n",
        "    self.Z2 = np.dot(self.H, self.W2) + self.b2\n",
        "    self.y_hat = self.Z2\n",
        "\n",
        "    return self.y_hat\n",
        "\n",
        "  def compute_loss(self, y_hat, y):\n",
        "    # Batch Mean Squared Error: 1/N * sum( 1/2 * (y_hat - y)^2 )\n",
        "    N = y.shape[0]\n",
        "    return np.sum(0.5 * (y_hat - y)**2) / N\n",
        "\n",
        "  def backward(self, y):\n",
        "    N = y.shape[0]\n",
        "\n",
        "    # dL/dÅ·\n",
        "    dy_hat = (self.y_hat - y) / N\n",
        "\n",
        "    # Output layer gradients\n",
        "    dW2 = np.dot(self.H.T, dy_hat)\n",
        "    db2 = np.sum(dy_hat, axis=0, keepdims=True)\n",
        "\n",
        "    # Hidden layer gradients\n",
        "    dH = np.dot(dy_hat, self.W2.T)\n",
        "    dZ1 = dH * self.H * (1 - self.H) # dH * sigmoid_derivative\n",
        "\n",
        "    dW1 = np.dot(self.X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}"
      ],
      "metadata": {
        "id": "kWTBC50P4kMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check(model, X, y, epsilon=1e-5, error_cutoff=1e-7):\n",
        "  print(\"--- Starting Gradient Check ---\\n\")\n",
        "\n",
        "  # 1. Run standard forward and backward pass\n",
        "  y_hat = model.forward(X)\n",
        "  analytic_grads = model.backward(y)\n",
        "\n",
        "  parameters = ['W1', 'b1', 'W2', 'b2']\n",
        "\n",
        "  for p_name in parameters:\n",
        "    param_array = getattr(model, p_name)\n",
        "    analytic_grad = analytic_grads[p_name]\n",
        "    numeric_grad = np.zeros_like(param_array)\n",
        "\n",
        "    # Iterate through each element in the parameter array\n",
        "    it = np.nditer(param_array, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "      idx = it.multi_index\n",
        "      orig_val = param_array[idx]\n",
        "\n",
        "      # Compute L(theta + epsilon)\n",
        "      param_array[idx] = orig_val + epsilon\n",
        "      y_hat_plus = model.forward(X)\n",
        "      loss_plus = model.compute_loss(y_hat_plus, y)\n",
        "\n",
        "      # Compute L(theta - epsilon)\n",
        "      param_array[idx] = orig_val - epsilon\n",
        "      y_hat_minus = model.forward(X)\n",
        "      loss_minus = model.compute_loss(y_hat_minus, y)\n",
        "\n",
        "      # Restore the original parameter value\n",
        "      param_array[idx] = orig_val\n",
        "\n",
        "      # Compute the numerical gradient for this specific parameter element\n",
        "      numeric_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "      it.iternext()\n",
        "\n",
        "      # Calculate relative error: ||analytic - numeric|| / (||analytic|| + ||numeric||)\n",
        "      numerator = np.linalg.norm(analytic_grad - numeric_grad)\n",
        "      denominator = np.linalg.norm(analytic_grad) + np.linalg.norm(numeric_grad)\n",
        "      rel_error = numerator / (denominator + 1e-15)\n",
        "\n",
        "      print(f\"Parameter {p_name}: relative error = {rel_error:.4e}\" + (\n",
        "          f\"  --> Warning: Gradient for {p_name} might be incorrect!\"\n",
        "          if rel_error > error_cutoff else\n",
        "          f\"  --> Success: Gradients for {p_name} match.\"\n",
        "      ))"
      ],
      "metadata": {
        "id": "JtCGJZQb7dTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultilayerPerceptron()\n",
        "gradient_check(model, X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfOOGOre7_I2",
        "outputId": "b6e43a68-180c-44d3-f121-d1118f6c4768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Gradient Check ---\n",
            "\n",
            "Parameter W1: relative error = 8.0397e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 6.3785e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 5.4129e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 5.3815e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 2.7120e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 2.6168e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 2.3261e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 2.0484e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 2.0372e-01  --> Warning: Gradient for W1 might be incorrect!\n",
            "Parameter W1: relative error = 1.8299e-10  --> Success: Gradients for W1 match.\n",
            "Parameter b1: relative error = 7.7049e-01  --> Warning: Gradient for b1 might be incorrect!\n",
            "Parameter b1: relative error = 5.8204e-01  --> Warning: Gradient for b1 might be incorrect!\n",
            "Parameter b1: relative error = 4.7352e-01  --> Warning: Gradient for b1 might be incorrect!\n",
            "Parameter b1: relative error = 4.6976e-01  --> Warning: Gradient for b1 might be incorrect!\n",
            "Parameter b1: relative error = 3.4248e-11  --> Success: Gradients for b1 match.\n",
            "Parameter W2: relative error = 6.0703e-01  --> Warning: Gradient for W2 might be incorrect!\n",
            "Parameter W2: relative error = 4.7687e-01  --> Warning: Gradient for W2 might be incorrect!\n",
            "Parameter W2: relative error = 3.4684e-01  --> Warning: Gradient for W2 might be incorrect!\n",
            "Parameter W2: relative error = 2.3217e-01  --> Warning: Gradient for W2 might be incorrect!\n",
            "Parameter W2: relative error = 5.8844e-12  --> Success: Gradients for W2 match.\n",
            "Parameter b2: relative error = 1.0700e-12  --> Success: Gradients for b2 match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsEczyJe_b5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}